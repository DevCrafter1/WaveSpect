# WaveSpect
__CFSD__: We have uploaded the complete CFSD dataset to the following link: https://pan.baidu.com/s/1Zq6umHVKmw3Yg2iPrX-gbA .<br>


__Real Dataset__: To collect real data, we gathered audio data from YouTube. We searched for different playlists related to education, news, entertainment, and philosophy to collect authentic audio for our dataset. Typically, speech datasets consist of single-channel audio data sampled at 16kHz. However, since our data comes from various sources, the sampling rate of the audio ranges between 8kHz and 44kHz. To standardize the data, we used the Librosa library to resample all audio to 16kHz.

__Synthetic Dataset__: We generated synthetic audio samples using eight different TTS systems. Below, we introduce these models: Bark, xTTS2, ChatTTS, EmotiVoice, GPT-SoVITs, MeloTTS, OpenVoice, and Vall-X-E are the eight speech synthesis models, each with distinct characteristics, covering areas from natural language processing and emotional expression to voice transformation and the integration of music and speech. <br>
__Bark__: a complete text-to-audio model that adopts a GPT-style architecture akin to AudioLM.  <br>
__xTTS2__: A lightweight, end-to-end TTS framework that generates high-quality, natural-sounding speech using advanced neural architectures like Transformers.  <br>
__ChatTTS__: AI-powered TTS for real-time conversational interactions, generating natural-sounding speech adapted to chatbots and live chats. <br>
__EmotiVoice__: A model that emphasizes emotional expression, suitable for emotional dialogue and multimedia applications.  <br>
__GPT-SoVITs__: Combines GPT and sound features, facilitating voice conversion.  <br>
__MeloTTS__: a deep-learning TTS tool that produces natural, expressive speech with customizable pitch, tone, and speed.  <br>
__OpenVoice__: An open TTS platform offering a diverse range of speech models <br>
__Vall-X-E__: A transformer-based, large-scale pre-trained model for high-quality speech synthesis. <br>

